{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(텐서플로우2 AND TPU)BERT LARGE WITH SQUAD V1.1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPLn1AbxqYhDv4i5yyXsZOw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f8c65835b5534abd8ebde9d4011a7b7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_77a9c7af977d4c919db06cbae5d6b586",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0063830657914ea1b5c208ac89092d17",
              "IPY_MODEL_73e89cc71fba4bc29251a88990ac146c"
            ]
          }
        },
        "77a9c7af977d4c919db06cbae5d6b586": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0063830657914ea1b5c208ac89092d17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_45af0230e6114509a7702b0c6ffbd11b",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_05f4f4ad878040a9948059f76c7dea31"
          }
        },
        "73e89cc71fba4bc29251a88990ac146c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_865c36ba522e4eef9fbe8faeb71b1445",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 232k/232k [00:00&lt;00:00, 2.62MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0cdd96d8ce0b413488f9c4caabe9dda3"
          }
        },
        "45af0230e6114509a7702b0c6ffbd11b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "05f4f4ad878040a9948059f76c7dea31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "865c36ba522e4eef9fbe8faeb71b1445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0cdd96d8ce0b413488f9c4caabe9dda3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b0af2c02aa3849d0b831baedbe61f299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d7c17b4c97624d4fad26367cfb95fb8a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_60ed8077955446b08dd702bb179dd172",
              "IPY_MODEL_3b6ca53e844a4985905b0508d562eb65"
            ]
          }
        },
        "d7c17b4c97624d4fad26367cfb95fb8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "60ed8077955446b08dd702bb179dd172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_53792a19c90147d69e730a381233e972",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 362,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 362,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_345147ad5c3940c9a75c447b353ae260"
          }
        },
        "3b6ca53e844a4985905b0508d562eb65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7554cc8a3d34403d89f2936edae5dbe3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 362/362 [00:00&lt;00:00, 18.1kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e9ccb8a9ec674701a4f06b0ce68f0d5f"
          }
        },
        "53792a19c90147d69e730a381233e972": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "345147ad5c3940c9a75c447b353ae260": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7554cc8a3d34403d89f2936edae5dbe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e9ccb8a9ec674701a4f06b0ce68f0d5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d60b7b77fb0d4d17b29c049b4a698d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d4944b74ddcb4c6e964b289f149237cd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ef99a99a07624d37ab0ee935eb9263f7",
              "IPY_MODEL_5bbb575917e6436884b6cc5591da00f3"
            ]
          }
        },
        "d4944b74ddcb4c6e964b289f149237cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ef99a99a07624d37ab0ee935eb9263f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8093050ca359449db2807f42e8596054",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1472569832,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1472569832,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5dedea8779264d9f9444639fda854bc5"
          }
        },
        "5bbb575917e6436884b6cc5591da00f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_84e198c186c54799bfcaca96de96fad6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 1.47G/1.47G [00:28&lt;00:00, 51.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_64b15994f38f4c34b074ffa063f8d960"
          }
        },
        "8093050ca359449db2807f42e8596054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5dedea8779264d9f9444639fda854bc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "84e198c186c54799bfcaca96de96fad6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "64b15994f38f4c34b074ffa063f8d960": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimwoonggon/publicservant_AI/blob/master/(%ED%85%90%EC%84%9C%ED%94%8C%EB%A1%9C%EC%9A%B02_AND_TPU)BERT_LARGE_WITH_SQUAD_V1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AB7YtQ3rZzF",
        "colab_type": "code",
        "outputId": "52c1c15a-8757-4dc3-dad4-23124eb3cc45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 3.4MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 16.8MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 33.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 55.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=049cf81804d1ac77a00801cdb593518d48fd69f66d777b9752a81eee23870076\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRXvtgWrrdk5",
        "colab_type": "code",
        "outputId": "384b964f-3c21-47a0-b18b-13100268dcd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import *\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JSqucXTB-jX",
        "colab_type": "code",
        "outputId": "91abe514-013e-4b36-c5c0-a5dd6ac9e412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMermJ31CMnx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"gdrive/My Drive/Colab Notebooks/squad\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4Xgl9RtsGul",
        "colab_type": "code",
        "outputId": "16eced92-6058-4d87-da3d-01a05040db2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68,
          "referenced_widgets": [
            "f8c65835b5534abd8ebde9d4011a7b7a",
            "77a9c7af977d4c919db06cbae5d6b586",
            "0063830657914ea1b5c208ac89092d17",
            "73e89cc71fba4bc29251a88990ac146c",
            "45af0230e6114509a7702b0c6ffbd11b",
            "05f4f4ad878040a9948059f76c7dea31",
            "865c36ba522e4eef9fbe8faeb71b1445",
            "0cdd96d8ce0b413488f9c4caabe9dda3"
          ]
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8c65835b5534abd8ebde9d4011a7b7a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUyl86xQ4rk6",
        "colab_type": "code",
        "outputId": "6635a6b5-2f79-4fb4-edec-a7868e802b25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/nate-parrott/squad/master/data/train-v1.1.json\n",
        "!wget https://raw.githubusercontent.com/nate-parrott/squad/master/data/dev-v1.1.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-07 11:52:17--  https://raw.githubusercontent.com/nate-parrott/squad/master/data/train-v1.1.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30288272 (29M) [text/plain]\n",
            "Saving to: ‘train-v1.1.json’\n",
            "\n",
            "train-v1.1.json     100%[===================>]  28.88M  45.8MB/s    in 0.6s    \n",
            "\n",
            "2020-03-07 11:52:18 (45.8 MB/s) - ‘train-v1.1.json’ saved [30288272/30288272]\n",
            "\n",
            "--2020-03-07 11:52:19--  https://raw.githubusercontent.com/nate-parrott/squad/master/data/dev-v1.1.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4854279 (4.6M) [text/plain]\n",
            "Saving to: ‘dev-v1.1.json’\n",
            "\n",
            "dev-v1.1.json       100%[===================>]   4.63M  22.6MB/s    in 0.2s    \n",
            "\n",
            "2020-03-07 11:52:20 (22.6 MB/s) - ‘dev-v1.1.json’ saved [4854279/4854279]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OqdYuQu8ukV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def squad_json_to_dataframe_train(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n",
        "                           verbose = 1):\n",
        "    \"\"\"\n",
        "    input_file_path: path to the squad json file.\n",
        "    record_path: path to deepest level in json file default value is\n",
        "    ['data','paragraphs','qas','answers']\n",
        "    verbose: 0 to suppress it default is 1\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"Reading the json file\")    \n",
        "    file = json.loads(open(input_file_path).read())\n",
        "    if verbose:\n",
        "        print(\"processing...\")\n",
        "    # parsing different level's in the json file\n",
        "    js = pd.io.json.json_normalize(file , record_path )\n",
        "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
        "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
        "    \n",
        "    #combining it into single dataframe\n",
        "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
        "    ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
        "    m['context'] = idx\n",
        "    js['q_idx'] = ndx\n",
        "    main = pd.concat([ m[['id','question','context']].set_index('id'),js.set_index('q_idx')],1,sort=False).reset_index()\n",
        "    main['c_id'] = main['context'].factorize()[0]\n",
        "    if verbose:\n",
        "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
        "        print(\"Done\")\n",
        "    return main"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWmRwNXE8xSb",
        "colab_type": "code",
        "outputId": "2c3c59a3-3285-4e6b-9f04-0da27a070bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "train = squad_json_to_dataframe_train(\"train-v1.1.json\")\n",
        "# context의 길이를 알려주는 칼럼 생성\n",
        "train['context_len'] = train['context'].str.len()\n",
        "# 질문의 길이가 10 미만인 데이터 확인\n",
        "# 질문의 길이가 10 미만이면 이상한 데이터일 가능성 높음\n",
        "train.loc[train['question'].str.len() <= 10].head(10)\n",
        "# 질문의 길이가 10 미만인 데이터 삭제\n",
        "train = train.loc[train['question'].str.len() >= 10].reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading the json file\n",
            "processing...\n",
            "shape of the dataframe is (87599, 6)\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt1HeYmI9FTL",
        "colab_type": "code",
        "outputId": "aff35c02-325b-4d51-8a82-2ee27de6e3ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def convert_data(data_df):\n",
        "    global tokenizer\n",
        "    indices, segments, masks, target_start, target_end = [], [], [], [], []\n",
        "    \n",
        "    for i in tqdm(range(len(data_df))):\n",
        "        # que : question을 버트 인풋으로 들어갈 수 있게 토큰화 한 변수, tokenizer.encode를 사용하면\n",
        "        # 토큰화 된 리스트와, 세그먼트가 같이 나오는데 일단 토큰화 된 리스트만 사용\n",
        "\n",
        "        # doc : context를 버트 인풋으로 들어갈 수 있게 토근화 한 변수\n",
        "        que = tokenizer.encode(data_df[QUESTION_COLUMN][i])\n",
        "        doc = tokenizer.encode(data_df[DATA_COLUMN][i])\n",
        "        \n",
        "        # 토큰화된 context의 맨 앞에 있는 [CLS]에 해당하는 101을 삭제\n",
        "        doc.pop(0)\n",
        "\n",
        "        # que_len, doc_len : 질문의 길이, context의 길이\n",
        "        que_len = len(que)\n",
        "        doc_len = len(doc)\n",
        "\n",
        "        # 만약 question의 길이가 64를 초과하면, 64로 잘라줌\n",
        "\n",
        "        if que_len > 64:\n",
        "          que = que[:63]\n",
        "          # 질문의 끝이 [SEP]이 되도록, [SEP]에 해당하는 3 추가\n",
        "          que.append(102)\n",
        "        \n",
        "        # 버트 인풋으로 들어가는 토큰화된 리스트가 최대 길이인 384가 넘지 않도록 만들어 줌\n",
        "        # 384 미만이면 context를 잘라줌\n",
        "        if len(que+doc) > SEQ_LEN:\n",
        "          while len(que+doc) != SEQ_LEN:\n",
        "            doc.pop(-1)\n",
        "          doc.pop(-1)\n",
        "          #context의 끝이 [SEP]가 되도록 [SEP]에 해당하는 102를 추가해 줌\n",
        "          doc.append(102)\n",
        "\n",
        "        # 문장의 전후관계를 구분해주는 segment는, question은 0이 되도록, context는 1이 되도록, 나머지 부분인 패딩 부분은\n",
        "        # 0이 되도록 만들어 줌\n",
        "        \n",
        "        ############################\n",
        "        ###### Segment 예시 ########\n",
        "        ############################\n",
        "        \n",
        "        # question, context, padding\n",
        "        # 00000000, 1111111, 0000000\n",
        "        \n",
        "        segment = [0]*len(que) + [1]*len(doc) + [0]*(SEQ_LEN-len(que)-len(doc))\n",
        "        if len(que + doc) <= SEQ_LEN:\n",
        "          mask = [1]*len(que+doc) + [0]*(SEQ_LEN-len(que+doc))\n",
        "        else:\n",
        "          mask = [1]*len(que+doc)\n",
        "        # 만약 question과 context를 합쳤을 때 그 길이가 384 미만이면\n",
        "        # padding 값인 0을 채워주도록 함\n",
        "        if len(que + doc) <= SEQ_LEN:\n",
        "          while len(que+doc) != SEQ_LEN:\n",
        "            doc.append(0)\n",
        "\n",
        "        # ids : question과 context를 합친 버트의 실질적인 인풋\n",
        "\n",
        "        ids = que + doc\n",
        "        \n",
        "        # text 길이만큼 context를 sliding 하면서, context 안에 일치하는 text를 찾았을 경우\n",
        "        # context 내에 text의 시작 위치와 끝 위치를 알려주는 부분 코딩\n",
        "        \n",
        "        text = tokenizer.encode(data_df[TEXT][i])\n",
        "        text_slide_len = len(text[1:-1])\n",
        "        \n",
        "        # exist_flag : context 내에서 text를 찾았을 경우 0에서 1로 전환\n",
        "        for j in range(0,(len(doc))):  \n",
        "            exist_flag = 0\n",
        "            if text[1:-1] == doc[j:j+text_slide_len]:\n",
        "              ans_start = j + len(que)\n",
        "              ans_end = j + text_slide_len - 1 + len(que)\n",
        "              exist_flag = 1\n",
        "              break\n",
        "        \n",
        "        # 만약 context 내에서 text를 찾지 못해서 여전히 exist_flag 가 0인 경우\n",
        "        # 시작값과 끝 값을 SEQ_LEN(384로 지정)\n",
        "        # 향후 시작값과 끝 값이 384인 경우 이 목록은 삭제할 예정임\n",
        "        if exist_flag == 0:\n",
        "          ans_start = SEQ_LEN\n",
        "          ans_end = SEQ_LEN\n",
        "\n",
        "        # 버트 인풋으로 들어가는 ids, segments를 indices, segments에 각각 저장\n",
        "        indices.append(ids)\n",
        "        segments.append(segment)\n",
        "        masks.append(mask)\n",
        "        # 정답에 해당하는 시작 위치인 ans_start와 ans_end를 target_start, target_end에 각각 저장\n",
        "        target_start.append(ans_start)\n",
        "        target_end.append(ans_end)\n",
        "\n",
        "    # indices, segments, ans_start, ans_end를 numpy array로 지정    \n",
        "    indices_x = np.array(indices)\n",
        "    segments = np.array(segments)\n",
        "    masks = np.array(masks)\n",
        "    target_start = np.array(target_start)\n",
        "    target_end = np.array(target_end)\n",
        "    \n",
        "    # del_list를 지정하여 ans_start와 ans_end가 정답에 해당하지 않는 부분들을 삭제\n",
        "    del_list = np.where(target_start!=SEQ_LEN)[0]\n",
        "    not_del_list = np.where(target_start==SEQ_LEN)[0]\n",
        "    indices_x = indices_x[del_list]\n",
        "    segments = segments[del_list]\n",
        "    masks = masks[del_list]\n",
        "    target_start = target_start[del_list]\n",
        "    target_end = target_end[del_list]\n",
        "\n",
        "    return [indices_x, masks, segments], [target_start, target_end], not_del_list\n",
        "\n",
        "# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n",
        "def load_data(pandas_dataframe):\n",
        "    data_df = pandas_dataframe\n",
        "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
        "    data_df[QUESTION_COLUMN] = data_df[QUESTION_COLUMN].astype(str)\n",
        "    data_df[TEXT] = data_df[TEXT].astype(str)\n",
        "    data_x, data_y, del_list = convert_data(data_df)\n",
        "\n",
        "    return data_x, data_y, del_list\n",
        "\n",
        "SEQ_LEN = 384\n",
        "DATA_COLUMN = \"context\"\n",
        "# context를 포함하고 있는 열의 이름\n",
        "QUESTION_COLUMN = \"question\"\n",
        "# question을 포함하고 있는 열의 이름\n",
        "TEXT = \"text\"\n",
        "# text(정답)을 포함하고 있는 열의 이름\n",
        "\n",
        "train_x, train_y, z = load_data(train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  4%|▎         | 3272/87589 [00:11<04:52, 287.99it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (718 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (718 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (718 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (718 > 512). Running this sequence through the model will result in indexing errors\n",
            " 23%|██▎       | 20505/87589 [01:03<03:26, 324.30it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
            " 30%|██▉       | 26054/87589 [01:20<04:08, 247.15it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
            " 34%|███▍      | 29884/87589 [01:35<04:33, 211.20it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n",
            " 41%|████      | 35687/87589 [01:56<03:16, 263.68it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n",
            " 47%|████▋     | 41129/87589 [02:17<02:29, 311.21it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
            " 56%|█████▌    | 49069/87589 [02:46<02:44, 234.67it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
            " 57%|█████▋    | 50183/87589 [02:50<02:24, 258.94it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
            " 58%|█████▊    | 50729/87589 [02:52<02:13, 276.04it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n",
            " 62%|██████▏   | 54304/87589 [03:05<01:58, 280.35it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors\n",
            " 66%|██████▌   | 57548/87589 [03:18<01:52, 268.05it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (767 > 512). Running this sequence through the model will result in indexing errors\n",
            " 66%|██████▌   | 57577/87589 [03:18<01:53, 264.61it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (767 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (767 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (767 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (767 > 512). Running this sequence through the model will result in indexing errors\n",
            " 68%|██████▊   | 59997/87589 [03:27<01:44, 265.20it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (853 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (853 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (853 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (853 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (853 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n",
            " 74%|███████▎  | 64547/87589 [03:43<01:24, 271.49it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (693 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (693 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (693 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (693 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (693 > 512). Running this sequence through the model will result in indexing errors\n",
            " 75%|███████▍  | 65687/87589 [03:47<01:17, 281.65it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
            " 76%|███████▌  | 66259/87589 [03:49<01:18, 273.37it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
            " 82%|████████▏ | 71758/87589 [04:10<00:58, 270.93it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n",
            " 94%|█████████▍| 82689/87589 [04:49<00:19, 249.65it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
            "100%|█████████▉| 87179/87589 [05:05<00:01, 299.14it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
            "100%|██████████| 87589/87589 [05:07<00:00, 285.07it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pavfaWfyKxR_",
        "colab_type": "code",
        "outputId": "d1339559-3bee-4e07-82fa-ba6ba9ba712f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: 10.50.196.2:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: 10.50.196.2:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.tpu.topology.Topology at 0x7f8f6ef5d390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISZmBp5ADuo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 깔끔한 모델\n",
        "def create_model2():\n",
        "  \n",
        "  model = TFBertModel.from_pretrained('bert-large-uncased')\n",
        "  token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids')\n",
        "  seg_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segments')\n",
        "  mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')\n",
        "\n",
        "  seq_output, _ = model([token_inputs, mask_inputs, seg_inputs])\n",
        "  x = tf.keras.layers.Dense(2, kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(seq_output)\n",
        "  start, end = tf.split(x, 2, axis=-1)\n",
        "  start = tf.squeeze(start, axis=-1)\n",
        "  end = tf.squeeze(end, axis=-1)\n",
        "  bert_model2 = tf.keras.Model([token_inputs, mask_inputs, seg_inputs], [start, end])\n",
        "  import tensorflow_addons as tfa\n",
        "  #opt = tfa.optimizers.RectifiedAdam(lr=5e-5, warmup_proportion=0.1, total_steps=10000)\n",
        "  opt = tf.keras.optimizers.Adam(lr=1.5e-5)\n",
        "  bert_model2.compile(\n",
        "      optimizer = opt,\n",
        "      loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      metrics=['sparse_categorical_accuracy'])\n",
        "  bert_model2.summary()\n",
        "  del model\n",
        "  return bert_model2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brEoCX9qLMsB",
        "colab_type": "code",
        "outputId": "6c7e305b-dc5e-4451-d916-d6f271817fa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b0af2c02aa3849d0b831baedbe61f299",
            "d7c17b4c97624d4fad26367cfb95fb8a",
            "60ed8077955446b08dd702bb179dd172",
            "3b6ca53e844a4985905b0508d562eb65",
            "53792a19c90147d69e730a381233e972",
            "345147ad5c3940c9a75c447b353ae260",
            "7554cc8a3d34403d89f2936edae5dbe3",
            "e9ccb8a9ec674701a4f06b0ce68f0d5f",
            "d60b7b77fb0d4d17b29c049b4a698d2c",
            "d4944b74ddcb4c6e964b289f149237cd",
            "ef99a99a07624d37ab0ee935eb9263f7",
            "5bbb575917e6436884b6cc5591da00f3",
            "8093050ca359449db2807f42e8596054",
            "5dedea8779264d9f9444639fda854bc5",
            "84e198c186c54799bfcaca96de96fad6",
            "64b15994f38f4c34b074ffa063f8d960"
          ]
        }
      },
      "source": [
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "with strategy.scope(): # creating the model in the TPUStrategy scope means we will train the model on the TPU\n",
        "  bert_model2 = create_model2()\n",
        "  \n",
        "bert_model2.fit(train_x, train_y, epochs=2, shuffle=True, batch_size=18)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0af2c02aa3849d0b831baedbe61f299",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=362, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d60b7b77fb0d4d17b29c049b4a698d2c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=1472569832, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_segments (InputLayer)     [(None, 384)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model (TFBertModel)     ((None, 384, 1024),  335141888   input_word_ids[0][0]             \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 input_segments[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 384, 2)       2050        tf_bert_model[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_split (TensorFlowOp [(None, 384, 1), (No 0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Squeeze (TensorFlow [(None, 384)]        0           tf_op_layer_split[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Squeeze_1 (TensorFl [(None, 384)]        0           tf_op_layer_split[0][1]          \n",
            "==================================================================================================\n",
            "Total params: 335,143,938\n",
            "Trainable params: 335,143,938\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 87354 samples\n",
            "Epoch 1/2\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "87354/87354 [==============================] - 1956s 22ms/sample - loss: 1.8067 - tf_op_layer_Squeeze_loss: 0.9459 - tf_op_layer_Squeeze_1_loss: 0.8609 - tf_op_layer_Squeeze_sparse_categorical_accuracy: 0.6366 - tf_op_layer_Squeeze_1_sparse_categorical_accuracy: 0.6793\n",
            "Epoch 2/2\n",
            "87354/87354 [==============================] - 1813s 21ms/sample - loss: 1.1158 - tf_op_layer_Squeeze_loss: 0.5970 - tf_op_layer_Squeeze_1_loss: 0.5188 - tf_op_layer_Squeeze_sparse_categorical_accuracy: 0.7457 - tf_op_layer_Squeeze_1_sparse_categorical_accuracy: 0.7900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8f70b97240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjINVmHN1FRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_model2.save_weights(os.path.join(path, \"bert_large_2epoch.h5\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e99oCCHKUHxj",
        "colab_type": "code",
        "outputId": "6f650704-e1a9-4844-979a-af2ad38bf76d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 2 EPOCH\n",
        "from sklearn.metrics import classification_report\n",
        "preds = bert_model2.predict(train_x)\n",
        "\n",
        "start_indexes = np.argmax(preds[0], axis=-1)\n",
        "end_indexes = np.argmax(preds[1], axis=-1)\n",
        "\n",
        "# start_index의 f1_score\n",
        "print(classification_report(train_y[0], start_indexes))\n",
        "\n",
        "# end_index의 f1_score\n",
        "print(classification_report(train_y[1], end_indexes))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           4       1.00      1.00      1.00         1\n",
            "           6       0.67      0.50      0.57         4\n",
            "           7       0.77      0.77      0.77        22\n",
            "           8       0.84      0.78      0.81        65\n",
            "           9       0.89      0.77      0.83       173\n",
            "          10       0.84      0.88      0.86       328\n",
            "          11       0.87      0.83      0.85       464\n",
            "          12       0.87      0.85      0.86       607\n",
            "          13       0.86      0.86      0.86       748\n",
            "          14       0.86      0.87      0.87       849\n",
            "          15       0.88      0.86      0.87       908\n",
            "          16       0.88      0.88      0.88       957\n",
            "          17       0.85      0.87      0.86       963\n",
            "          18       0.86      0.85      0.86       993\n",
            "          19       0.85      0.86      0.86      1001\n",
            "          20       0.84      0.87      0.85       972\n",
            "          21       0.86      0.86      0.86       988\n",
            "          22       0.85      0.87      0.86       986\n",
            "          23       0.86      0.86      0.86       938\n",
            "          24       0.86      0.86      0.86       932\n",
            "          25       0.85      0.87      0.86       959\n",
            "          26       0.86      0.83      0.85       911\n",
            "          27       0.84      0.85      0.84       910\n",
            "          28       0.85      0.88      0.86       894\n",
            "          29       0.84      0.87      0.85       900\n",
            "          30       0.83      0.83      0.83       871\n",
            "          31       0.84      0.83      0.84       822\n",
            "          32       0.87      0.86      0.86       871\n",
            "          33       0.84      0.86      0.85       821\n",
            "          34       0.84      0.86      0.85       799\n",
            "          35       0.83      0.84      0.84       818\n",
            "          36       0.86      0.83      0.85       826\n",
            "          37       0.85      0.84      0.84       810\n",
            "          38       0.84      0.85      0.85       778\n",
            "          39       0.84      0.85      0.85       790\n",
            "          40       0.86      0.83      0.85       797\n",
            "          41       0.83      0.85      0.84       730\n",
            "          42       0.84      0.84      0.84       751\n",
            "          43       0.84      0.85      0.84       777\n",
            "          44       0.84      0.85      0.84       734\n",
            "          45       0.84      0.84      0.84       760\n",
            "          46       0.84      0.83      0.83       772\n",
            "          47       0.85      0.85      0.85       735\n",
            "          48       0.83      0.85      0.84       749\n",
            "          49       0.83      0.85      0.84       720\n",
            "          50       0.85      0.84      0.85       719\n",
            "          51       0.84      0.85      0.84       675\n",
            "          52       0.84      0.85      0.85       719\n",
            "          53       0.85      0.84      0.84       671\n",
            "          54       0.86      0.85      0.85       703\n",
            "          55       0.83      0.85      0.84       669\n",
            "          56       0.85      0.81      0.83       689\n",
            "          57       0.83      0.84      0.83       627\n",
            "          58       0.84      0.84      0.84       659\n",
            "          59       0.84      0.82      0.83       700\n",
            "          60       0.83      0.82      0.83       672\n",
            "          61       0.82      0.86      0.84       646\n",
            "          62       0.84      0.84      0.84       659\n",
            "          63       0.85      0.84      0.85       615\n",
            "          64       0.84      0.83      0.84       595\n",
            "          65       0.87      0.85      0.86       656\n",
            "          66       0.87      0.83      0.85       620\n",
            "          67       0.83      0.84      0.83       588\n",
            "          68       0.83      0.85      0.84       574\n",
            "          69       0.85      0.87      0.86       603\n",
            "          70       0.84      0.83      0.83       585\n",
            "          71       0.84      0.82      0.83       600\n",
            "          72       0.85      0.83      0.84       605\n",
            "          73       0.84      0.83      0.84       607\n",
            "          74       0.86      0.86      0.86       570\n",
            "          75       0.86      0.85      0.86       589\n",
            "          76       0.82      0.82      0.82       543\n",
            "          77       0.81      0.85      0.83       517\n",
            "          78       0.85      0.84      0.84       559\n",
            "          79       0.85      0.84      0.84       548\n",
            "          80       0.85      0.85      0.85       543\n",
            "          81       0.83      0.82      0.82       508\n",
            "          82       0.84      0.83      0.83       526\n",
            "          83       0.86      0.84      0.85       572\n",
            "          84       0.85      0.86      0.86       501\n",
            "          85       0.83      0.88      0.85       492\n",
            "          86       0.84      0.82      0.83       516\n",
            "          87       0.83      0.86      0.85       516\n",
            "          88       0.84      0.82      0.83       487\n",
            "          89       0.86      0.85      0.85       476\n",
            "          90       0.83      0.84      0.84       505\n",
            "          91       0.82      0.83      0.83       463\n",
            "          92       0.83      0.86      0.84       499\n",
            "          93       0.81      0.81      0.81       468\n",
            "          94       0.82      0.84      0.83       482\n",
            "          95       0.84      0.86      0.85       481\n",
            "          96       0.82      0.85      0.84       431\n",
            "          97       0.84      0.87      0.85       465\n",
            "          98       0.80      0.83      0.81       455\n",
            "          99       0.80      0.79      0.80       456\n",
            "         100       0.85      0.83      0.84       488\n",
            "         101       0.83      0.82      0.82       451\n",
            "         102       0.82      0.82      0.82       482\n",
            "         103       0.81      0.83      0.82       452\n",
            "         104       0.79      0.81      0.80       457\n",
            "         105       0.82      0.81      0.81       445\n",
            "         106       0.83      0.83      0.83       433\n",
            "         107       0.84      0.81      0.82       432\n",
            "         108       0.81      0.82      0.82       409\n",
            "         109       0.85      0.82      0.83       437\n",
            "         110       0.83      0.83      0.83       421\n",
            "         111       0.83      0.85      0.84       401\n",
            "         112       0.83      0.84      0.84       397\n",
            "         113       0.83      0.83      0.83       418\n",
            "         114       0.82      0.80      0.81       388\n",
            "         115       0.82      0.84      0.83       373\n",
            "         116       0.83      0.81      0.82       389\n",
            "         117       0.83      0.83      0.83       372\n",
            "         118       0.83      0.80      0.81       410\n",
            "         119       0.83      0.84      0.83       369\n",
            "         120       0.84      0.84      0.84       381\n",
            "         121       0.81      0.82      0.82       355\n",
            "         122       0.81      0.79      0.80       341\n",
            "         123       0.83      0.85      0.84       350\n",
            "         124       0.82      0.83      0.83       365\n",
            "         125       0.84      0.84      0.84       297\n",
            "         126       0.86      0.80      0.83       343\n",
            "         127       0.87      0.85      0.86       317\n",
            "         128       0.82      0.83      0.83       320\n",
            "         129       0.83      0.85      0.84       298\n",
            "         130       0.82      0.86      0.84       286\n",
            "         131       0.88      0.82      0.85       311\n",
            "         132       0.82      0.81      0.82       295\n",
            "         133       0.85      0.83      0.84       297\n",
            "         134       0.83      0.83      0.83       302\n",
            "         135       0.81      0.79      0.80       281\n",
            "         136       0.81      0.84      0.83       305\n",
            "         137       0.83      0.85      0.84       278\n",
            "         138       0.84      0.80      0.82       244\n",
            "         139       0.85      0.85      0.85       253\n",
            "         140       0.88      0.88      0.88       247\n",
            "         141       0.88      0.85      0.87       235\n",
            "         142       0.86      0.83      0.84       248\n",
            "         143       0.82      0.85      0.84       219\n",
            "         144       0.82      0.83      0.83       206\n",
            "         145       0.84      0.84      0.84       213\n",
            "         146       0.85      0.84      0.84       177\n",
            "         147       0.82      0.79      0.81       209\n",
            "         148       0.83      0.85      0.84       196\n",
            "         149       0.83      0.86      0.85       185\n",
            "         150       0.88      0.81      0.84       216\n",
            "         151       0.81      0.81      0.81       201\n",
            "         152       0.84      0.85      0.85       177\n",
            "         153       0.86      0.79      0.82       196\n",
            "         154       0.79      0.82      0.80       182\n",
            "         155       0.82      0.88      0.85       166\n",
            "         156       0.90      0.85      0.87       165\n",
            "         157       0.81      0.85      0.83       174\n",
            "         158       0.84      0.79      0.82       143\n",
            "         159       0.83      0.80      0.82       152\n",
            "         160       0.91      0.81      0.86       156\n",
            "         161       0.78      0.81      0.79       141\n",
            "         162       0.81      0.81      0.81       149\n",
            "         163       0.87      0.79      0.83       163\n",
            "         164       0.80      0.85      0.82       142\n",
            "         165       0.75      0.76      0.75       139\n",
            "         166       0.81      0.81      0.81       113\n",
            "         167       0.87      0.80      0.84       156\n",
            "         168       0.82      0.83      0.82       145\n",
            "         169       0.79      0.82      0.80       120\n",
            "         170       0.87      0.82      0.84       133\n",
            "         171       0.83      0.86      0.84       106\n",
            "         172       0.87      0.83      0.85       115\n",
            "         173       0.82      0.85      0.84       109\n",
            "         174       0.83      0.82      0.83       104\n",
            "         175       0.86      0.81      0.83       111\n",
            "         176       0.76      0.81      0.79        97\n",
            "         177       0.82      0.85      0.83       111\n",
            "         178       0.84      0.82      0.83       102\n",
            "         179       0.82      0.83      0.82        92\n",
            "         180       0.83      0.81      0.82        91\n",
            "         181       0.81      0.84      0.82        94\n",
            "         182       0.88      0.85      0.87       102\n",
            "         183       0.84      0.84      0.84        92\n",
            "         184       0.90      0.90      0.90       100\n",
            "         185       0.91      0.84      0.87       104\n",
            "         186       0.87      0.91      0.89        78\n",
            "         187       0.91      0.82      0.87        90\n",
            "         188       0.88      0.89      0.88        94\n",
            "         189       0.82      0.83      0.82        76\n",
            "         190       0.82      0.90      0.86        81\n",
            "         191       0.83      0.75      0.79        89\n",
            "         192       0.86      0.91      0.88        85\n",
            "         193       0.91      0.84      0.87        74\n",
            "         194       0.86      0.84      0.85        80\n",
            "         195       0.88      0.80      0.83        79\n",
            "         196       0.70      0.82      0.76        51\n",
            "         197       0.88      0.80      0.84        83\n",
            "         198       0.79      0.82      0.80        65\n",
            "         199       0.83      0.87      0.85        69\n",
            "         200       0.84      0.80      0.82        61\n",
            "         201       0.78      0.81      0.80        48\n",
            "         202       0.81      0.69      0.75        62\n",
            "         203       0.92      0.89      0.91        66\n",
            "         204       0.79      0.86      0.82        56\n",
            "         205       0.78      0.87      0.82        61\n",
            "         206       0.79      0.76      0.78        50\n",
            "         207       0.86      0.86      0.86        43\n",
            "         208       0.91      0.86      0.88        56\n",
            "         209       0.75      0.77      0.76        61\n",
            "         210       0.84      0.82      0.83        57\n",
            "         211       0.87      0.85      0.86        54\n",
            "         212       0.74      0.81      0.78        43\n",
            "         213       0.84      0.80      0.82        46\n",
            "         214       0.83      0.89      0.86        45\n",
            "         215       0.91      0.89      0.90        36\n",
            "         216       0.76      0.93      0.84        30\n",
            "         217       0.89      0.82      0.85        39\n",
            "         218       0.82      0.80      0.81        40\n",
            "         219       0.84      0.80      0.82        51\n",
            "         220       0.85      0.87      0.86        38\n",
            "         221       0.79      0.82      0.81        33\n",
            "         222       0.77      0.83      0.80        41\n",
            "         223       0.84      0.71      0.77        45\n",
            "         224       0.81      0.83      0.82        35\n",
            "         225       0.88      0.90      0.89        42\n",
            "         226       0.82      0.91      0.86        34\n",
            "         227       0.81      0.75      0.78        28\n",
            "         228       0.94      0.77      0.85        22\n",
            "         229       0.88      0.85      0.86        33\n",
            "         230       0.95      0.86      0.90        42\n",
            "         231       0.79      0.88      0.83        34\n",
            "         232       0.87      0.79      0.83        33\n",
            "         233       0.77      0.88      0.82        26\n",
            "         234       0.87      0.80      0.84        41\n",
            "         235       0.81      0.79      0.80        33\n",
            "         236       0.83      0.81      0.82        36\n",
            "         237       0.94      0.85      0.89        20\n",
            "         238       0.89      0.80      0.84        30\n",
            "         239       0.84      0.87      0.85        30\n",
            "         240       0.86      0.96      0.91        25\n",
            "         241       0.84      0.84      0.84        25\n",
            "         242       0.88      0.83      0.86        36\n",
            "         243       0.88      0.88      0.88        40\n",
            "         244       0.82      0.88      0.85        26\n",
            "         245       0.93      0.85      0.89        33\n",
            "         246       0.89      0.92      0.91        26\n",
            "         247       0.92      0.92      0.92        26\n",
            "         248       0.91      0.95      0.93        21\n",
            "         249       0.93      0.81      0.87        16\n",
            "         250       0.71      0.83      0.77        12\n",
            "         251       0.75      0.86      0.80        14\n",
            "         252       0.67      0.67      0.67         9\n",
            "         253       0.85      1.00      0.92        17\n",
            "         254       1.00      0.88      0.94        17\n",
            "         255       0.94      0.81      0.87        21\n",
            "         256       0.87      1.00      0.93        13\n",
            "         257       0.91      0.77      0.83        26\n",
            "         258       0.73      0.89      0.80         9\n",
            "         259       0.80      0.86      0.83        14\n",
            "         260       0.72      0.76      0.74        17\n",
            "         261       0.93      0.87      0.90        15\n",
            "         262       1.00      0.85      0.92        13\n",
            "         263       0.75      0.90      0.82        10\n",
            "         264       0.80      0.92      0.86        13\n",
            "         265       0.92      0.69      0.79        16\n",
            "         266       0.81      0.93      0.87        14\n",
            "         267       1.00      0.73      0.84        11\n",
            "         268       0.83      0.83      0.83         6\n",
            "         269       0.91      0.83      0.87        12\n",
            "         270       0.75      0.92      0.83        13\n",
            "         271       0.90      0.64      0.75        14\n",
            "         272       0.86      0.90      0.88        20\n",
            "         273       0.73      0.69      0.71        16\n",
            "         274       0.79      0.79      0.79        14\n",
            "         275       0.75      0.75      0.75         8\n",
            "         276       0.80      1.00      0.89        12\n",
            "         277       1.00      0.73      0.84        11\n",
            "         278       0.90      1.00      0.95         9\n",
            "         279       0.75      0.86      0.80         7\n",
            "         280       0.86      0.75      0.80         8\n",
            "         281       0.75      0.90      0.82        10\n",
            "         282       0.71      0.71      0.71         7\n",
            "         283       1.00      1.00      1.00        11\n",
            "         284       0.92      0.86      0.89        14\n",
            "         285       0.88      0.70      0.78        10\n",
            "         286       0.89      1.00      0.94         8\n",
            "         287       0.80      1.00      0.89         8\n",
            "         288       0.80      0.67      0.73        12\n",
            "         289       0.92      1.00      0.96        11\n",
            "         290       0.64      0.90      0.75        10\n",
            "         291       0.50      1.00      0.67         1\n",
            "         292       1.00      1.00      1.00         5\n",
            "         293       0.83      0.83      0.83         6\n",
            "         294       1.00      0.80      0.89         5\n",
            "         295       0.67      1.00      0.80         6\n",
            "         296       0.86      0.86      0.86         7\n",
            "         297       1.00      1.00      1.00         6\n",
            "         298       1.00      0.71      0.83         7\n",
            "         299       0.83      0.83      0.83         6\n",
            "         300       1.00      0.50      0.67         2\n",
            "         301       1.00      0.73      0.84        11\n",
            "         302       1.00      0.78      0.88         9\n",
            "         303       0.88      0.88      0.88         8\n",
            "         304       0.50      1.00      0.67         2\n",
            "         305       0.71      0.71      0.71         7\n",
            "         306       0.83      0.83      0.83         6\n",
            "         307       1.00      0.67      0.80         3\n",
            "         308       0.60      1.00      0.75         3\n",
            "         309       1.00      1.00      1.00         2\n",
            "         310       0.75      0.75      0.75         4\n",
            "         311       0.89      1.00      0.94         8\n",
            "         312       0.75      1.00      0.86         3\n",
            "         313       0.90      0.90      0.90        10\n",
            "         314       0.67      0.33      0.44         6\n",
            "         315       0.86      1.00      0.92         6\n",
            "         316       1.00      0.67      0.80         6\n",
            "         317       0.75      1.00      0.86         3\n",
            "         318       1.00      1.00      1.00         2\n",
            "         319       1.00      0.82      0.90        11\n",
            "         320       0.67      1.00      0.80         2\n",
            "         321       1.00      1.00      1.00         3\n",
            "         322       0.80      0.80      0.80         5\n",
            "         323       1.00      0.67      0.80         3\n",
            "         324       0.67      0.67      0.67         6\n",
            "         325       1.00      1.00      1.00         1\n",
            "         326       1.00      0.71      0.83         7\n",
            "         327       0.40      1.00      0.57         2\n",
            "         328       1.00      0.80      0.89         5\n",
            "         329       1.00      1.00      1.00         3\n",
            "         330       1.00      1.00      1.00         5\n",
            "         331       1.00      0.50      0.67         2\n",
            "         332       1.00      1.00      1.00         5\n",
            "         333       0.83      1.00      0.91         5\n",
            "         334       1.00      0.80      0.89         5\n",
            "         335       0.80      0.67      0.73         6\n",
            "         336       1.00      1.00      1.00         2\n",
            "         337       0.80      1.00      0.89         4\n",
            "         338       0.75      1.00      0.86         3\n",
            "         339       1.00      0.75      0.86         4\n",
            "         340       0.60      1.00      0.75         3\n",
            "         341       1.00      0.71      0.83         7\n",
            "         342       1.00      1.00      1.00         1\n",
            "         343       0.50      1.00      0.67         1\n",
            "         344       1.00      1.00      1.00         1\n",
            "         345       0.00      0.00      0.00         0\n",
            "         346       1.00      1.00      1.00         3\n",
            "         347       1.00      1.00      1.00         2\n",
            "         348       1.00      0.33      0.50         3\n",
            "         349       0.50      1.00      0.67         2\n",
            "         350       0.50      0.50      0.50         2\n",
            "         351       1.00      1.00      1.00         1\n",
            "         352       1.00      0.80      0.89         5\n",
            "         353       0.75      1.00      0.86         3\n",
            "         354       1.00      0.75      0.86         4\n",
            "         355       0.80      0.80      0.80         5\n",
            "         356       1.00      1.00      1.00         2\n",
            "         357       1.00      1.00      1.00         2\n",
            "         358       0.00      0.00      0.00         2\n",
            "         359       0.60      1.00      0.75         3\n",
            "         360       0.50      0.50      0.50         2\n",
            "         361       0.50      0.50      0.50         2\n",
            "         362       1.00      0.50      0.67         2\n",
            "         363       0.67      0.67      0.67         3\n",
            "         364       0.60      0.75      0.67         4\n",
            "         365       1.00      0.67      0.80         3\n",
            "         366       1.00      0.67      0.80         3\n",
            "         367       1.00      1.00      1.00         3\n",
            "         368       0.50      0.50      0.50         2\n",
            "         369       0.00      0.00      0.00         0\n",
            "         370       1.00      1.00      1.00         1\n",
            "         371       1.00      1.00      1.00         1\n",
            "         372       1.00      0.67      0.80         3\n",
            "         373       1.00      1.00      1.00         1\n",
            "         375       1.00      1.00      1.00         1\n",
            "         376       1.00      1.00      1.00         3\n",
            "         377       1.00      1.00      1.00         1\n",
            "         378       1.00      1.00      1.00         1\n",
            "         381       1.00      1.00      1.00         1\n",
            "         382       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.84     87354\n",
            "   macro avg       0.84      0.83      0.83     87354\n",
            "weighted avg       0.84      0.84      0.84     87354\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           6       1.00      1.00      1.00         2\n",
            "           7       0.67      1.00      0.80         2\n",
            "           8       1.00      1.00      1.00        13\n",
            "           9       1.00      0.96      0.98        46\n",
            "          10       0.95      0.96      0.95       108\n",
            "          11       0.93      0.93      0.93       233\n",
            "          12       0.93      0.94      0.94       341\n",
            "          13       0.93      0.94      0.93       465\n",
            "          14       0.90      0.94      0.92       560\n",
            "          15       0.91      0.92      0.92       684\n",
            "          16       0.90      0.92      0.91       774\n",
            "          17       0.91      0.93      0.92       836\n",
            "          18       0.91      0.92      0.91       848\n",
            "          19       0.89      0.94      0.91       873\n",
            "          20       0.90      0.91      0.90       902\n",
            "          21       0.90      0.91      0.91       901\n",
            "          22       0.91      0.91      0.91       934\n",
            "          23       0.89      0.90      0.90       933\n",
            "          24       0.89      0.91      0.90       903\n",
            "          25       0.91      0.91      0.91       911\n",
            "          26       0.88      0.88      0.88       883\n",
            "          27       0.88      0.89      0.89       866\n",
            "          28       0.88      0.90      0.89       920\n",
            "          29       0.89      0.88      0.88       907\n",
            "          30       0.90      0.88      0.89       929\n",
            "          31       0.89      0.90      0.89       859\n",
            "          32       0.89      0.90      0.89       795\n",
            "          33       0.87      0.90      0.89       820\n",
            "          34       0.88      0.88      0.88       828\n",
            "          35       0.89      0.89      0.89       828\n",
            "          36       0.88      0.88      0.88       851\n",
            "          37       0.88      0.88      0.88       826\n",
            "          38       0.88      0.88      0.88       794\n",
            "          39       0.87      0.89      0.88       804\n",
            "          40       0.89      0.87      0.88       788\n",
            "          41       0.89      0.89      0.89       778\n",
            "          42       0.87      0.88      0.88       796\n",
            "          43       0.87      0.88      0.87       797\n",
            "          44       0.88      0.86      0.87       772\n",
            "          45       0.87      0.89      0.88       747\n",
            "          46       0.88      0.88      0.88       754\n",
            "          47       0.87      0.90      0.89       742\n",
            "          48       0.90      0.90      0.90       788\n",
            "          49       0.89      0.90      0.89       757\n",
            "          50       0.86      0.87      0.87       717\n",
            "          51       0.87      0.87      0.87       694\n",
            "          52       0.87      0.88      0.87       705\n",
            "          53       0.88      0.89      0.89       690\n",
            "          54       0.88      0.85      0.87       689\n",
            "          55       0.87      0.90      0.88       724\n",
            "          56       0.89      0.86      0.88       720\n",
            "          57       0.87      0.88      0.87       633\n",
            "          58       0.89      0.88      0.89       665\n",
            "          59       0.88      0.89      0.88       689\n",
            "          60       0.88      0.90      0.89       629\n",
            "          61       0.89      0.88      0.89       709\n",
            "          62       0.90      0.87      0.89       679\n",
            "          63       0.89      0.89      0.89       709\n",
            "          64       0.86      0.86      0.86       599\n",
            "          65       0.86      0.88      0.87       660\n",
            "          66       0.89      0.87      0.88       670\n",
            "          67       0.88      0.86      0.87       621\n",
            "          68       0.86      0.87      0.87       583\n",
            "          69       0.89      0.89      0.89       632\n",
            "          70       0.86      0.90      0.88       591\n",
            "          71       0.88      0.87      0.87       602\n",
            "          72       0.89      0.89      0.89       615\n",
            "          73       0.89      0.86      0.87       612\n",
            "          74       0.89      0.88      0.88       597\n",
            "          75       0.89      0.88      0.89       554\n",
            "          76       0.90      0.86      0.88       570\n",
            "          77       0.89      0.87      0.88       564\n",
            "          78       0.86      0.87      0.87       547\n",
            "          79       0.87      0.84      0.86       559\n",
            "          80       0.86      0.87      0.87       573\n",
            "          81       0.88      0.90      0.89       563\n",
            "          82       0.88      0.85      0.86       487\n",
            "          83       0.87      0.87      0.87       557\n",
            "          84       0.87      0.89      0.88       528\n",
            "          85       0.88      0.85      0.87       506\n",
            "          86       0.89      0.85      0.87       542\n",
            "          87       0.86      0.86      0.86       496\n",
            "          88       0.86      0.84      0.85       510\n",
            "          89       0.85      0.87      0.86       503\n",
            "          90       0.87      0.87      0.87       511\n",
            "          91       0.85      0.88      0.87       474\n",
            "          92       0.87      0.83      0.85       493\n",
            "          93       0.84      0.88      0.86       467\n",
            "          94       0.85      0.87      0.86       508\n",
            "          95       0.85      0.87      0.86       488\n",
            "          96       0.84      0.85      0.85       457\n",
            "          97       0.84      0.85      0.84       462\n",
            "          98       0.85      0.86      0.85       469\n",
            "          99       0.86      0.85      0.85       436\n",
            "         100       0.85      0.87      0.86       479\n",
            "         101       0.85      0.88      0.87       502\n",
            "         102       0.85      0.86      0.86       471\n",
            "         103       0.85      0.87      0.86       463\n",
            "         104       0.87      0.87      0.87       491\n",
            "         105       0.86      0.86      0.86       467\n",
            "         106       0.85      0.89      0.87       444\n",
            "         107       0.84      0.82      0.83       441\n",
            "         108       0.86      0.84      0.85       436\n",
            "         109       0.86      0.86      0.86       431\n",
            "         110       0.88      0.86      0.87       445\n",
            "         111       0.87      0.87      0.87       403\n",
            "         112       0.87      0.86      0.87       417\n",
            "         113       0.82      0.82      0.82       370\n",
            "         114       0.84      0.86      0.85       423\n",
            "         115       0.85      0.84      0.84       434\n",
            "         116       0.86      0.85      0.86       376\n",
            "         117       0.85      0.85      0.85       411\n",
            "         118       0.86      0.83      0.84       385\n",
            "         119       0.85      0.84      0.84       407\n",
            "         120       0.86      0.88      0.87       380\n",
            "         121       0.88      0.85      0.87       393\n",
            "         122       0.84      0.82      0.83       343\n",
            "         123       0.87      0.83      0.85       373\n",
            "         124       0.84      0.86      0.85       357\n",
            "         125       0.82      0.85      0.83       362\n",
            "         126       0.85      0.84      0.84       372\n",
            "         127       0.88      0.86      0.87       346\n",
            "         128       0.87      0.88      0.87       334\n",
            "         129       0.84      0.85      0.85       323\n",
            "         130       0.83      0.83      0.83       314\n",
            "         131       0.87      0.84      0.85       333\n",
            "         132       0.89      0.85      0.87       329\n",
            "         133       0.90      0.89      0.90       324\n",
            "         134       0.90      0.84      0.86       317\n",
            "         135       0.83      0.83      0.83       315\n",
            "         136       0.88      0.82      0.85       307\n",
            "         137       0.89      0.87      0.88       293\n",
            "         138       0.84      0.84      0.84       290\n",
            "         139       0.88      0.85      0.86       282\n",
            "         140       0.87      0.90      0.89       253\n",
            "         141       0.86      0.85      0.85       273\n",
            "         142       0.84      0.83      0.83       242\n",
            "         143       0.85      0.86      0.85       258\n",
            "         144       0.86      0.88      0.87       234\n",
            "         145       0.85      0.82      0.83       227\n",
            "         146       0.85      0.82      0.83       227\n",
            "         147       0.87      0.83      0.85       226\n",
            "         148       0.85      0.87      0.86       210\n",
            "         149       0.83      0.82      0.82       195\n",
            "         150       0.86      0.85      0.85       224\n",
            "         151       0.80      0.85      0.82       203\n",
            "         152       0.86      0.81      0.83       225\n",
            "         153       0.85      0.78      0.82       188\n",
            "         154       0.85      0.85      0.85       192\n",
            "         155       0.89      0.88      0.88       187\n",
            "         156       0.89      0.85      0.87       188\n",
            "         157       0.90      0.88      0.89       185\n",
            "         158       0.88      0.90      0.89       159\n",
            "         159       0.89      0.85      0.87       166\n",
            "         160       0.82      0.85      0.83       171\n",
            "         161       0.89      0.87      0.88       163\n",
            "         162       0.87      0.84      0.86       161\n",
            "         163       0.84      0.85      0.84       137\n",
            "         164       0.88      0.88      0.88       161\n",
            "         165       0.85      0.87      0.86       158\n",
            "         166       0.85      0.83      0.84       139\n",
            "         167       0.85      0.84      0.84       160\n",
            "         168       0.90      0.87      0.88       130\n",
            "         169       0.85      0.82      0.83       140\n",
            "         170       0.88      0.86      0.87       134\n",
            "         171       0.91      0.82      0.86       142\n",
            "         172       0.89      0.89      0.89       131\n",
            "         173       0.89      0.90      0.90       119\n",
            "         174       0.87      0.86      0.87       110\n",
            "         175       0.84      0.80      0.82       115\n",
            "         176       0.76      0.85      0.80       108\n",
            "         177       0.88      0.87      0.87       123\n",
            "         178       0.86      0.87      0.86       102\n",
            "         179       0.86      0.90      0.88       102\n",
            "         180       0.87      0.86      0.87       108\n",
            "         181       0.83      0.83      0.83        99\n",
            "         182       0.86      0.82      0.84       108\n",
            "         183       0.84      0.84      0.84        93\n",
            "         184       0.89      0.78      0.83       110\n",
            "         185       0.87      0.92      0.90       112\n",
            "         186       0.81      0.81      0.81        80\n",
            "         187       0.83      0.80      0.81        79\n",
            "         188       0.90      0.91      0.90       104\n",
            "         189       0.85      0.88      0.86        75\n",
            "         190       0.86      0.90      0.88        80\n",
            "         191       0.85      0.83      0.84        96\n",
            "         192       0.89      0.88      0.89        86\n",
            "         193       0.85      0.86      0.86        72\n",
            "         194       0.86      0.84      0.85        77\n",
            "         195       0.82      0.81      0.82        85\n",
            "         196       0.88      0.80      0.84        83\n",
            "         197       0.87      0.85      0.86        86\n",
            "         198       0.86      0.84      0.85        79\n",
            "         199       0.87      0.87      0.87        77\n",
            "         200       0.84      0.87      0.85        67\n",
            "         201       0.87      0.87      0.87        69\n",
            "         202       0.87      0.79      0.83        82\n",
            "         203       0.76      0.85      0.80        60\n",
            "         204       0.87      0.78      0.82        60\n",
            "         205       0.89      0.89      0.89        65\n",
            "         206       0.91      0.89      0.90        55\n",
            "         207       0.85      0.76      0.80        51\n",
            "         208       0.82      0.80      0.81        50\n",
            "         209       0.92      0.80      0.86        61\n",
            "         210       0.90      0.88      0.89        75\n",
            "         211       0.88      0.93      0.90        54\n",
            "         212       0.92      0.88      0.90        51\n",
            "         213       0.98      0.83      0.90        53\n",
            "         214       0.86      0.91      0.88        46\n",
            "         215       0.78      0.87      0.82        45\n",
            "         216       0.87      0.85      0.86        47\n",
            "         217       0.73      0.77      0.75        39\n",
            "         218       0.87      0.92      0.89        37\n",
            "         219       0.86      0.80      0.83        45\n",
            "         220       0.83      0.83      0.83        52\n",
            "         221       0.83      0.84      0.84        45\n",
            "         222       0.88      0.72      0.79        53\n",
            "         223       0.78      0.74      0.76        42\n",
            "         224       0.89      0.84      0.86        37\n",
            "         225       0.78      0.81      0.79        26\n",
            "         226       0.86      0.86      0.86        36\n",
            "         227       0.77      0.80      0.79        30\n",
            "         228       0.85      0.93      0.89        43\n",
            "         229       0.88      0.88      0.88        24\n",
            "         230       0.94      0.88      0.91        33\n",
            "         231       0.78      0.83      0.81        30\n",
            "         232       0.83      0.83      0.83        42\n",
            "         233       0.85      0.82      0.84        28\n",
            "         234       0.90      0.82      0.86        34\n",
            "         235       0.87      0.79      0.83        33\n",
            "         236       0.86      0.79      0.82        38\n",
            "         237       0.86      0.90      0.88        42\n",
            "         238       0.86      0.86      0.86        28\n",
            "         239       0.83      0.92      0.87        26\n",
            "         240       0.92      0.82      0.87        28\n",
            "         241       0.81      0.84      0.82        25\n",
            "         242       0.81      0.84      0.83        31\n",
            "         243       0.86      0.70      0.78        27\n",
            "         244       0.85      0.89      0.87        38\n",
            "         245       0.93      0.87      0.90        31\n",
            "         246       0.95      1.00      0.97        35\n",
            "         247       0.86      0.89      0.88        28\n",
            "         248       0.95      0.86      0.90        43\n",
            "         249       0.83      0.80      0.81        30\n",
            "         250       0.74      0.81      0.77        21\n",
            "         251       0.86      0.86      0.86        14\n",
            "         252       0.80      0.67      0.73        18\n",
            "         253       0.82      0.88      0.85        16\n",
            "         254       0.92      0.79      0.85        14\n",
            "         255       0.76      0.94      0.84        17\n",
            "         256       0.88      0.50      0.64        14\n",
            "         257       0.85      0.92      0.88        24\n",
            "         258       0.71      0.71      0.71        17\n",
            "         259       0.75      0.92      0.83        13\n",
            "         260       1.00      0.81      0.90        16\n",
            "         261       1.00      1.00      1.00        19\n",
            "         262       0.93      0.93      0.93        15\n",
            "         263       0.75      0.92      0.83        13\n",
            "         264       0.81      1.00      0.89        17\n",
            "         265       0.92      0.79      0.85        14\n",
            "         266       0.94      0.89      0.92        19\n",
            "         267       0.78      0.88      0.82         8\n",
            "         268       0.80      0.80      0.80        10\n",
            "         269       0.94      0.85      0.89        20\n",
            "         270       1.00      1.00      1.00         9\n",
            "         271       0.80      0.80      0.80         5\n",
            "         272       0.79      0.88      0.83        17\n",
            "         273       0.93      0.93      0.93        14\n",
            "         274       0.86      0.67      0.75         9\n",
            "         275       0.88      0.94      0.91        16\n",
            "         276       0.77      1.00      0.87        10\n",
            "         277       0.92      0.71      0.80        17\n",
            "         278       0.60      0.75      0.67         8\n",
            "         279       0.82      0.75      0.78        12\n",
            "         280       0.57      0.67      0.62         6\n",
            "         281       0.57      0.80      0.67         5\n",
            "         282       0.82      0.75      0.78        12\n",
            "         283       1.00      0.90      0.95        10\n",
            "         284       0.84      0.94      0.89        17\n",
            "         285       0.90      0.82      0.86        11\n",
            "         286       0.85      1.00      0.92        11\n",
            "         287       0.89      0.73      0.80        11\n",
            "         288       0.89      0.73      0.80        11\n",
            "         289       0.89      0.89      0.89         9\n",
            "         290       0.88      0.88      0.88         8\n",
            "         291       1.00      1.00      1.00        11\n",
            "         292       0.90      1.00      0.95         9\n",
            "         293       1.00      1.00      1.00         3\n",
            "         294       1.00      1.00      1.00         4\n",
            "         295       0.83      0.83      0.83         6\n",
            "         296       0.86      1.00      0.92         6\n",
            "         297       1.00      0.86      0.92         7\n",
            "         298       0.83      0.71      0.77         7\n",
            "         299       0.78      1.00      0.88         7\n",
            "         300       1.00      0.88      0.93         8\n",
            "         301       0.80      1.00      0.89         4\n",
            "         302       0.75      0.75      0.75         4\n",
            "         303       0.80      0.67      0.73        12\n",
            "         304       0.78      0.70      0.74        10\n",
            "         305       1.00      1.00      1.00         6\n",
            "         306       1.00      0.80      0.89         5\n",
            "         307       1.00      1.00      1.00         4\n",
            "         308       0.70      1.00      0.82         7\n",
            "         309       1.00      1.00      1.00         4\n",
            "         310       0.00      0.00      0.00         1\n",
            "         311       1.00      0.67      0.80         3\n",
            "         312       0.50      0.83      0.62         6\n",
            "         313       1.00      0.60      0.75         5\n",
            "         314       1.00      0.89      0.94         9\n",
            "         315       1.00      0.62      0.77         8\n",
            "         316       1.00      1.00      1.00         4\n",
            "         317       0.75      0.86      0.80         7\n",
            "         318       1.00      1.00      1.00         4\n",
            "         319       1.00      0.86      0.92         7\n",
            "         320       0.75      0.75      0.75         4\n",
            "         321       0.57      0.80      0.67         5\n",
            "         322       1.00      0.67      0.80         3\n",
            "         323       1.00      1.00      1.00         3\n",
            "         324       1.00      0.86      0.92         7\n",
            "         325       0.80      1.00      0.89         4\n",
            "         326       1.00      1.00      1.00         4\n",
            "         328       1.00      1.00      1.00         3\n",
            "         329       0.86      0.86      0.86         7\n",
            "         330       1.00      1.00      1.00         7\n",
            "         331       1.00      1.00      1.00         3\n",
            "         332       0.75      0.60      0.67         5\n",
            "         333       1.00      0.60      0.75         5\n",
            "         334       0.50      1.00      0.67         1\n",
            "         335       1.00      1.00      1.00         4\n",
            "         336       1.00      1.00      1.00         5\n",
            "         337       1.00      1.00      1.00         3\n",
            "         338       1.00      1.00      1.00         2\n",
            "         339       0.50      0.75      0.60         4\n",
            "         340       0.86      0.86      0.86         7\n",
            "         341       1.00      1.00      1.00         2\n",
            "         342       0.75      0.75      0.75         4\n",
            "         343       1.00      1.00      1.00         1\n",
            "         344       0.75      1.00      0.86         3\n",
            "         345       0.00      0.00      0.00         2\n",
            "         346       0.71      1.00      0.83         5\n",
            "         347       0.67      1.00      0.80         2\n",
            "         348       0.00      0.00      0.00         2\n",
            "         349       0.50      0.50      0.50         2\n",
            "         350       0.50      0.50      0.50         2\n",
            "         351       1.00      0.67      0.80         3\n",
            "         352       0.83      1.00      0.91         5\n",
            "         353       0.50      0.67      0.57         3\n",
            "         354       1.00      0.50      0.67         2\n",
            "         355       1.00      0.67      0.80         3\n",
            "         356       1.00      1.00      1.00         2\n",
            "         357       1.00      0.67      0.80         3\n",
            "         358       1.00      0.75      0.86         4\n",
            "         359       0.50      1.00      0.67         1\n",
            "         360       1.00      0.50      0.67         2\n",
            "         361       0.00      0.00      0.00         0\n",
            "         362       1.00      1.00      1.00         2\n",
            "         363       0.33      1.00      0.50         1\n",
            "         364       0.00      0.00      0.00         3\n",
            "         365       1.00      1.00      1.00         4\n",
            "         366       1.00      0.80      0.89         5\n",
            "         367       1.00      0.50      0.67         4\n",
            "         368       0.00      0.00      0.00         1\n",
            "         369       0.50      1.00      0.67         2\n",
            "         370       1.00      0.50      0.67         2\n",
            "         371       0.50      0.50      0.50         2\n",
            "         372       1.00      0.33      0.50         3\n",
            "         373       1.00      1.00      1.00         3\n",
            "         374       0.67      1.00      0.80         2\n",
            "         375       1.00      0.50      0.67         2\n",
            "         377       1.00      1.00      1.00         1\n",
            "         378       1.00      1.00      1.00         2\n",
            "         379       1.00      0.50      0.67         2\n",
            "         380       0.00      0.00      0.00         1\n",
            "         381       1.00      1.00      1.00         1\n",
            "         382       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           0.87     87354\n",
            "   macro avg       0.85      0.84      0.84     87354\n",
            "weighted avg       0.88      0.87      0.87     87354\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53xGaFvUt7gr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def squad_json_to_dataframe_dev(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n",
        "                           verbose = 1):\n",
        "    \"\"\"\n",
        "    input_file_path: path to the squad json file.\n",
        "    record_path: path to deepest level in json file default value is\n",
        "    ['data','paragraphs','qas','answers']\n",
        "    verbose: 0 to suppress it default is 1\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"Reading the json file\")    \n",
        "    file = json.loads(open(input_file_path).read())\n",
        "    if verbose:\n",
        "        print(\"processing...\")\n",
        "    # parsing different level's in the json file\n",
        "    js = pd.io.json.json_normalize(file , record_path )\n",
        "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
        "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
        "    \n",
        "    #combining it into single dataframe\n",
        "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
        "    m['context'] = idx\n",
        "    main = m[['id','question','context','answers']].set_index('id').reset_index()\n",
        "    main['c_id'] = main['context'].factorize()[0]\n",
        "    if verbose:\n",
        "        print(\"shape of the dataframe is {}\".format(main.shape))\n",
        "        print(\"Done\")\n",
        "    return main"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_4U59gwt8cx",
        "colab_type": "code",
        "outputId": "25bba303-9c42-4c4a-df9c-06d128eea026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "input_file_path ='dev-v1.1.json'\n",
        "record_path = ['data','paragraphs','qas','answers']\n",
        "verbose = 0\n",
        "dev = squad_json_to_dataframe_dev(input_file_path=input_file_path,record_path=record_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading the json file\n",
            "processing...\n",
            "shape of the dataframe is (10570, 5)\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTqyoK-Pt9U0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 정답의 개수를 정의하는 칼럼 생성\n",
        "dev['answer_len'] = dev['answers'].map(lambda x: len(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gMZKxFat_T5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 정답들을 다루기 쉽게 리스트로 반환하는 함수 정의\n",
        "def get_text(text_len, answers):\n",
        "  # text_len : 질문(question)과 문장(context)에 해당하는 정답의 개수\n",
        "  # answers : 정답 ex) [{'answer_start': 177, 'text': 'Denver Broncos'}, {'answer_start': 177, 'text': 'Denver Broncos'}, {'answer_start': 177, 'text': 'Denver Broncos'}]\n",
        "  texts = []\n",
        "  for i in range(text_len):\n",
        "    texts.append(answers[i]['text'])\n",
        "  return texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FDKF-B8uBAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# texts 칼럼의 모든 데이터에 대해서 수행\n",
        "dev['texts'] = dev.apply(lambda x: get_text(x['answer_len'], x['answers']), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4FzpiScuCTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT_COLUMN = 'texts'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mlZHBCIuD1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_data(data_df):\n",
        "    global tokenizer\n",
        "    indices, segments, masks, target_start, target_end = [], [], [], [], []\n",
        "\n",
        "    for i in tqdm(range(len(data_df))):\n",
        "        que = tokenizer.encode(data_df[QUESTION_COLUMN][i])\n",
        "        doc = tokenizer.encode(data_df[DATA_COLUMN][i])\n",
        "        doc.pop(0)\n",
        "\n",
        "        que_len = len(que)\n",
        "        doc_len = len(doc)\n",
        "\n",
        "        if que_len > 64:\n",
        "          que = que[:63]\n",
        "          que.append(102)\n",
        "        \n",
        "        if len(que+doc) > SEQ_LEN:\n",
        "          while len(que+doc) != SEQ_LEN:\n",
        "            doc.pop(-1)\n",
        "\n",
        "          doc.pop(-1)\n",
        "          doc.append(102)\n",
        "        \n",
        "        if len(que + doc) <= SEQ_LEN:\n",
        "          mask = [1]*len(que+doc) + [0]*(SEQ_LEN-len(que+doc))\n",
        "        else:\n",
        "          mask = [1]*len(que+doc)\n",
        "        segment = [0]*len(que) + [1]*len(doc) + [0]*(SEQ_LEN-len(que)-len(doc))\n",
        "        if len(que + doc) <= SEQ_LEN:\n",
        "          while len(que+doc) != SEQ_LEN:\n",
        "            doc.append(0)\n",
        "\n",
        "        ids = que + doc\n",
        "\n",
        "        texts = data_df[TEXT_COLUMN][i]\n",
        "        for text_element in texts:\n",
        "          text = tokenizer.encode(text_element)\n",
        "\n",
        "          text_slide_len = len(text[1:-1])\n",
        "          for j in range(0,(len(doc))):  \n",
        "              exist_flag = 0\n",
        "              if text[1:-1] == doc[j:j+text_slide_len]:\n",
        "                ans_start = j + len(que)\n",
        "                ans_end = j + text_slide_len - 1 + len(que)\n",
        "                exist_flag = 1\n",
        "                break\n",
        "        \n",
        "          if exist_flag == 0:\n",
        "            ans_start = SEQ_LEN\n",
        "            ans_end = SEQ_LEN\n",
        "\n",
        "        indices.append(ids)\n",
        "        segments.append(segment)\n",
        "        masks.append(mask)\n",
        "        target_start.append(ans_start)\n",
        "        target_end.append(ans_end)\n",
        "        \n",
        "\n",
        "\n",
        "    # indices, segments, ans_start, ans_end를 numpy array로 지정    \n",
        "    indices_x = np.array(indices)\n",
        "    segments = np.array(segments)\n",
        "    masks = np.array(masks)\n",
        "    target_start = np.array(target_start)\n",
        "    target_end = np.array(target_end)\n",
        "    \n",
        "    # del_list를 지정하여 ans_start와 ans_end가 정답에 해당하지 않는 부분들을 삭제\n",
        "    del_list = np.where(target_start!=SEQ_LEN)[0]\n",
        "    not_del_list = np.where(target_start==SEQ_LEN)[0]\n",
        "    indices_x = indices_x[del_list]\n",
        "    segments = segments[del_list]\n",
        "    masks = masks[del_list]\n",
        "    target_start = target_start[del_list]\n",
        "    target_end = target_end[del_list]\n",
        "\n",
        "    return [indices_x, masks, segments], del_list\n",
        "\n",
        "def load_data(pandas_dataframe):\n",
        "    data_df = pandas_dataframe\n",
        "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
        "    data_df[QUESTION_COLUMN] = data_df[QUESTION_COLUMN].astype(str)\n",
        "    data_x, data_y, del_list = convert_data(data_df)\n",
        "\n",
        "    return data_x, del_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBtLSyq2uHqR",
        "colab_type": "code",
        "outputId": "44edf87d-f60d-4e32-d827-02ba54724519",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        }
      },
      "source": [
        "dev_bert_input = convert_data(dev)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 39%|███▉      | 4123/10570 [00:14<00:29, 215.52it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
            " 39%|███▉      | 4146/10570 [00:14<00:36, 177.69it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
            " 39%|███▉      | 4166/10570 [00:14<00:41, 153.09it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
            " 40%|███▉      | 4183/10570 [00:15<00:44, 142.70it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
            " 40%|███▉      | 4199/10570 [00:15<00:48, 132.13it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
            " 40%|████      | 4256/10570 [00:15<00:39, 160.42it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (644 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (644 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (644 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (644 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (644 > 512). Running this sequence through the model will result in indexing errors\n",
            " 40%|████      | 4274/10570 [00:15<00:49, 127.80it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (700 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (700 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (700 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (700 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (700 > 512). Running this sequence through the model will result in indexing errors\n",
            " 41%|████      | 4289/10570 [00:15<00:51, 121.09it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (789 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (789 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (789 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (789 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
            " 41%|████      | 4303/10570 [00:15<00:56, 111.42it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
            " 46%|████▌     | 4843/10570 [00:18<00:22, 253.05it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (733 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (733 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (733 > 512). Running this sequence through the model will result in indexing errors\n",
            "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (733 > 512). Running this sequence through the model will result in indexing errors\n",
            "100%|██████████| 10570/10570 [00:41<00:00, 253.43it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnxDJ_GqyctH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "dev_bert_input, del_list = dev_bert_input[0], dev_bert_input[1]\n",
        "dev = dev.iloc[del_list]\n",
        "dev = dev.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6erbnJ7yeLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indexes = dev_bert_input[0]\n",
        "bert_predictions = bert_model2.predict(dev_bert_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNlLL0U_yi6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_indexes = np.argmax(bert_predictions[0], axis=-1)\n",
        "end_indexes = np.argmax(bert_predictions[1], axis=-1)\n",
        "not_del_list = np.where(start_indexes <= end_indexes)[0]\n",
        "start_indexes = start_indexes[not_del_list]\n",
        "end_indexes = end_indexes[not_del_list]\n",
        "indexes = indexes[not_del_list]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQdJTf7Jym_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev = dev.iloc[not_del_list].reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bQ0BCvqzRNk",
        "colab_type": "code",
        "outputId": "75a7e586-a126-4174-eee0-98e3c21ee859",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "tokenizer.convert_ids_to_tokens(102)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[SEP]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX9kFqvOyoZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# length : dev 데이터의 길이\n",
        "length = len(dev)\n",
        "\n",
        "sentences = []\n",
        "\n",
        "untokenized = []\n",
        "\n",
        "for j in range(len(start_indexes)):\n",
        "  sentence = []\n",
        "  for i in range(start_indexes[j], end_indexes[j]+1):\n",
        "    token_based_word = tokenizer.convert_ids_to_tokens(indexes[j][i].item())\n",
        "    sentence.append(token_based_word)\n",
        "    # 문장이 토큰화된 단어 하나 하나를 sentence에 저장\n",
        "  \n",
        "  sentence_string = \"\"\n",
        "  \n",
        "  for w in sentence:\n",
        "    \n",
        "    if w.startswith(\"##\"):\n",
        "      w = w.replace(\"##\", \"\")\n",
        "      # 만약 sentence 안의 토큰이 ##으로 시작한다면, ##을 제거\n",
        "    else:\n",
        "      w = \" \" + w\n",
        "      # 토큰이 ##으로 시작하지 않는다면 글자의 첫 시작이므로, 띄어쓰기 추가\n",
        "    sentence_string += w\n",
        "      # 리스트로 되어 있는 토큰들을 하나로 합쳐줌\n",
        "  if sentence_string.startswith(\" \"):\n",
        "    sentence_string = \"\" + sentence_string[1:]\n",
        "    # sentence_string이 \" \"로 시작하는 경우에는 띄어쓰기를 삭제\n",
        "  untokenized.append(sentence_string)\n",
        "  # 리스트로 되어있는 토큰들을 하나로 합쳐준 것, 이것을 untokenized에 저장\n",
        "  sentences.append(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqs1FMc60A9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "dev_answers = []\n",
        "for i in range(length):\n",
        "  dev_answer = []\n",
        "  texts_dict = dev['answers'][i]\n",
        "  \n",
        "  for j in range(len(texts_dict)):\n",
        "    dev_answer.append(texts_dict[j]['text'])\n",
        "    # 정답 하나 하나를 리스트로 저장\n",
        "  dev_answers.append(dev_answer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TECtErHo0Crp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_tokens = []\n",
        "for i in dev_answers:\n",
        "  dev_tokened = []\n",
        "  for j in i:\n",
        "    temp_token = tokenizer.tokenize(j)\n",
        "    #print(temp_token)\n",
        "    # 정답을 토큰화\n",
        "    #temp_token.pop(0)\n",
        "    # [CLS] 제거\n",
        "    #temp_token.pop(-1)\n",
        "    # [SEP] 제거\n",
        "    dev_tokened.append(temp_token)\n",
        "  dev_tokens.append(dev_tokened)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGZIomHN0LYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 토큰화된 정답을 문장으로 변환시켜주고 합쳐줌\n",
        "dev_answer_lists = []\n",
        "for dev_answers in dev_tokens:\n",
        "  dev_answer_list = []\n",
        "  for dev_answer in dev_answers:\n",
        "    dev_answer_string = \" \".join(dev_answer)\n",
        "    dev_answer_list.append(dev_answer_string)\n",
        "  dev_answer_lists.append(dev_answer_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YexMh_Fn0cj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# untokenizing\n",
        "dev_strings_end = []\n",
        "for dev_strings in dev_answer_lists:\n",
        "  dev_strings_processed = []\n",
        "  for dev_string in dev_strings:\n",
        "    dev_string = dev_string.replace(\" ##\", \"\")\n",
        "    dev_strings_processed.append(dev_string)\n",
        "  dev_strings_end.append(dev_strings_processed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPOHmpwK0fWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_answers = dev_strings_end"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEK_4ZOz0g5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import string, re\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9s5VPKa0igE",
        "colab_type": "code",
        "outputId": "670b635b-deca-4982-e7b3-c41474fc0c0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "f1_sum = 0\n",
        "\n",
        "for i in range(len(untokenized)):\n",
        "  f1 = metric_max_over_ground_truths(f1_score, untokenized[i], dev_answers[i])\n",
        "  f1_sum += f1\n",
        "print(\"f1 score : \", f1_sum / length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1 score :  0.9039740554897687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4fASW8-0jXB",
        "colab_type": "code",
        "outputId": "82e26c76-1bdb-47ef-d501-e052d09758ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "EM_sum = 0\n",
        "\n",
        "for i in range(len(untokenized)):\n",
        "  \n",
        "  EM = metric_max_over_ground_truths(exact_match_score, untokenized[i], dev_answers[i])\n",
        "  EM_sum += EM\n",
        "print(\"EM Score : \", EM_sum / length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EM Score :  0.8289740372550912\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
